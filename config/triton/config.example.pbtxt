# Triton Inference Server Configuration Example

# Server settings
model_repository: "/models"
model_control_mode: "explicit"  # explicit or poll
strict_model_config: true

# Logging
log_verbose: 0  # 0-3, higher = more verbose
log_info: true
log_warning: true
log_error: true

# Backend settings
backend_config:
  pytorch:
    # PyTorch backend settings
    enable_torch_compile: false
  
  onnxruntime:
    # ONNX Runtime settings
    enable_cuda: true

# Resource limits
max_queue_delay_microseconds: 100000

# HTTP/gRPC settings
http_port: 8000
grpc_port: 8001
metrics_port: 8002

# GPU settings
# Triton will use all available GPUs by default
# To restrict GPU usage:
# cuda_visible_devices: "0,1"  # Use GPUs 0 and 1

# Rate limiter
rate_limiter:
  enabled: false
  # Configuration here

# Tracing (for debugging)
trace_file: "/tmp/triton_trace.log"
trace_level: "OFF"  # OFF, TIMESTAMPS, TENSORS
