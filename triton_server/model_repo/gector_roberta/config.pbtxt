name: "gector_roberta"
backend: "python"
max_batch_size: 8

input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [-1]
  },
  {
    name: "attention_mask"
    data_type: TYPE_INT64
    dims: [-1]
  }
]

output [
  {
    name: "logits"
    data_type: TYPE_FP32
    dims: [-1, -1]
  }
]

# Instance groups - configure based on available resources
# For GPU deployment:
instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [0]
  }
]

# For CPU deployment (uncomment the section below and comment GPU section above):
# instance_group [
#   {
#     count: 1
#     kind: KIND_CPU
#   }
# ]

# Python backend requires these parameters
parameters: {
  key: "EXECUTION_ENV_PATH",
  value: {string_value: "/opt/tritonserver/python_backend_env"}
}

# Dynamic batching for better throughput
dynamic_batching {
  preferred_batch_size: [1, 2, 4, 8]
  max_queue_delay_microseconds: 100000
}

# Model versioning
version_policy: {
  specific {
    versions: [1]
  }
}
