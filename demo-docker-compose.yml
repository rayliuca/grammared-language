# Docker Compose configuration for Grammared Language
# This is a placeholder/template for orchestrating the services

services:
  # Triton Inference Server
  triton-server:
    image: rayliuca/grammared-language-triton-arm:latest
    container_name: grammared-triton
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - MODEL_REPOSITORY=/models
      - LOG_VERBOSE=0
      - TRANSFORMERS_CACHE=/cache
      - HF_HOME=/cache
      # Gector Deberta Large Model Configuration
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__TYPE=gector
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__BACKEND=triton
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__TRITON_HOST=triton-server
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__TRITON_PORT=8001
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__PRETRAINED_MODEL_NAME_OR_PATH=gotutiyan/gector-deberta-large-5k
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__TRITON_MODEL_NAME=gector_deberta_large
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__DEVICE=cpu
      # CoEdit Large Model Configuration
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__TYPE=coedit
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__BACKEND=triton
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__TRITON_HOST=triton-server
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__TRITON_PORT=8001
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__PRETRAINED_MODEL_NAME_OR_PATH=rayliuca/coedit-large-onnx-quantized
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__TRITON_MODEL_NAME=coedit_large
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__BACKEND=ort
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__DEVICE=cpu
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__MODEL_INIT_CONFIG__BATCH_SIZE=4
      - "GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__GRAMMARED_CONFIG__PROMPT_TEMPLATE=Fix grammatical errors: {{ text }}"
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ./config/triton:/config:ro
      - hf-cache:/cache  # HuggingFace model cache
      # - ./model_config.yaml:/model_config.yaml:ro  # Model configuration
    # shm_size: '1gb'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - grammared-network
    restart: unless-stopped

  # API Service
  api-service:
    image: rayliuca/grammared-language-api:latest
    container_name: grammared-api
    ports:
      - "50051:50051"
      - "50052:50052"
    volumes:
      - ./logs:/app/logs
      - ./data:/data
      # - ./docker/default_model_config.yaml:/model_config.yaml:ro
    environment:
      # - GRRAMMARED_LANGUAGE__API_PORT=50051
      - LOG_LEVEL=INFO
      - GRAMMARED_LANGUAGE__MODEL_CONFIG_PATH=/model_config.yaml
      # Gector Deberta Large Model Configuration
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__TYPE=gector
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__BACKEND=triton
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__TRITON_HOST=triton-server
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__TRITON_PORT=8001
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__PRETRAINED_MODEL_NAME_OR_PATH=gotutiyan/gector-deberta-large-5k
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__TRITON_MODEL_NAME=gector_deberta_large
      - GRAMMARED_LANGUAGE__MODELS__GECTOR_DEBERTA_LARGE__SERVING_CONFIG__DEVICE=cpu
      # CoEdit Large Model Configuration
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__TYPE=coedit
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__BACKEND=triton
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__TRITON_HOST=triton-server
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__TRITON_PORT=8001
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__PRETRAINED_MODEL_NAME_OR_PATH=rayliuca/coedit-large-onnx-quantized
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__TRITON_MODEL_NAME=coedit_large
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__BACKEND=ort
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__SERVING_CONFIG__DEVICE=cpu
      - GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__MODEL_INIT_CONFIG__BATCH_SIZE=4
      - "GRAMMARED_LANGUAGE__MODELS__COEDIT_LARGE__GRAMMARED_CONFIG__PROMPT_TEMPLATE=Fix grammatical errors: {{ text }}"
    depends_on:
      triton-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:50052/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - grammared-network
    restart: unless-stopped
  languagetool:
    image: erikvl87/languagetool:latest
    container_name: languagetool
    restart: unless-stopped
    ports:
      - 8081:8010
    environment:
      download_ngrams_for_langs: en
      MAP_UID: 1001
      MAP_GID: 1001
      langtool_remoteRulesFile: /remote-rule-config.json
      Java_Xms: 512m
      Java_Xmx: 2g
    volumes:
      # mkdir -p ./ngrams && wget -O ./ngrams/ngrams-en-20150817.zip https://languagetool.org/download/ngram-data/ngrams-en-20150817.zip && unzip ./ngrams/ngrams-en-20150817.zip -d ./ngrams && rm ./ngrams/ngrams-en-20150817.zip
      - ./ngrams:/ngrams:ro
      - ./example_language_tool_configs/remote-rule-config.json:/remote-rule-config.json:ro
    networks:
      - grammared-network
      - grammared-cloudflared
    depends_on:
      triton-server:
        condition: service_healthy
      api-service:
        condition: service_healthy
  cloudflared:
    image: cloudflare/cloudflared
    container_name: grammared_language_cloudflared
    environment:
      - TZ=America/Edmonton # Change this to your timezone
      - TUNNEL_TOKEN=${GRAMMARED_LANGUAGE__CLOUDFLARED_TUNNEL_TOKEN}
    restart: unless-stopped
    command: tunnel --no-autoupdate run --token ${GRAMMARED_LANGUAGE__CLOUDFLARED_TUNNEL_TOKEN}
    networks:
      - grammared-cloudflared
    depends_on:
      - languagetool
networks:
  grammared-network:
  grammared-cloudflared:
    name: grammared-cloudflared
# Volumes for persistent data
volumes:
  hf-cache:
    driver: local